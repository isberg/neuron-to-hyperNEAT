{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Neuron notebook we looked at the single neuron and how it could be simulated given input `X`, weights `W`, bias `b` and an activation function `f`. We saw how we could reduce the number of lines of code using vector operations and how by showing the weights and bias in a clever way could make the neuron calculate the function we wanted in a few cases. We also managed to connect multiple neurons and make the network calculate the `XOR` of 2 inputs, something not possible with a single neuron.\n",
    "\n",
    "It was a bit of a hassle and quite a few lines of code with the small network and to verify that it did what we wanted. The goal in this notebook is to be able to calculate the output of a Artificial Neural Network in a easier way than before, and also to be able to verify how correct the output is. We will still do the maths ourselves, using vector operations and the numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have an input layer with 4 inputs, a hidden layer with 3 neurons and an output layer with 2 neurons. We use `X` for the input values as before and `Y` for the output result. Ideally we would like to use `W` to represent the weights and `b` for the bias, but now we have two layers of neurons with multiple number of neurons in each layer, so we must make sure we don't confuse ourselves to much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation we could use the following variables:\n",
    "\n",
    "```python\n",
    "X = [x1, x2, x3, x4]  # the inputs\n",
    "Wh1 = [?, ?, ?, ?]    # the weights of the first neuron in the hidden layer connected to all 4 inputs\n",
    "Wh2 = [?, ?, ?, ?]    # the weights of the second neuron in the hidden layer connected to all 4 inputs\n",
    "Wh3 = [?, ?, ?, ?]    # the weights of the third neuron in the hidden layer connected to all 4 inputs\n",
    "Wo1 = [?, ?, ?]    # the weights of the first neuron in the output layer connected to all 3 hidden neurons\n",
    "Wo2 = [?, ?, ?]    # the weights of the second neuron in the output layer connected to all 3 hidden neurons\n",
    "bh1, bh2, bh3, bo1, bo2 = ? # the bias of our 5 neurons\n",
    "```\n",
    "\n",
    "To calculate the the output from the first neuron in the hidden layer we could do:\n",
    "\n",
    "```python\n",
    "zh1 = np.dot(X, W) + b\n",
    "h1 = step(zh1)\n",
    "```\n",
    "\n",
    "It would be a bit laboursome to type this code for all the neurons though, and it wouldn't scale well. So, how can we do it more convenient, but still very clear. We could do similar to in the Neuron notebook:\n",
    "\n",
    "```python\n",
    "def g(X, W):\n",
    "    z = np.dot(X, W)\n",
    "    return step(z)\n",
    "```\n",
    "\n",
    "and we could create a function that evaluate the full network like this:\n",
    "\n",
    "```python\n",
    "def G(X, W1, W2, W3):\n",
    "    h1 = g(X, W1)\n",
    "    h2 = g(X, W2)\n",
    "    y = g([h1, h2], W3)\n",
    "    return y\n",
    "```\n",
    "\n",
    "Note that we did not need the bias in that example and therefore it is skipped in the above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to create thos kind of functions for our current network. (By accident we forgot to consider the usage for this network, but we will ignore that for now and focuse on the structure and the code to simulate it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(z):\n",
    "    return 1 if z > 0 else 0\n",
    "\n",
    "def g(X, W, b):\n",
    "    z = np.dot(X, W) + b\n",
    "    return step(z)\n",
    "\n",
    "def G(X, Wh1, Wh2, Wh3, bh1, bh2, bh3, Wo1, Wo2, bo1, bo2):\n",
    "    h1 = g(X, Wh1, bh1)\n",
    "    h2 = g(X, Wh2, bh2)\n",
    "    h3 = g(X, Wh3, bh3)\n",
    "    h = [h1, h2, h3]\n",
    "    y1 = g(h, Wo1, bo1)\n",
    "    y2 = g(h, Wo2, bo2)\n",
    "    y = [y1, y2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully `G` now correctly calculates the output of our 3 layer network (an input layer with 4 inputs, a hidden layer with 3 neurons and an output layer with 2 neurons), but yikes are there many parameters. Lets try to call the `G` and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=[1, 0, 1, 0]  ->  Y=[0, 1]\n",
      "X=[1, 1, 1, 1]  ->  Y=[1, 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # to have deterministic random values :-)\n",
    "Wh1 = np.random.normal(size=4)\n",
    "Wh2 = np.random.normal(size=4)\n",
    "Wh3 = np.random.normal(size=4)\n",
    "bh1 = np.random.normal()\n",
    "bh2 = np.random.normal()\n",
    "bh3 = np.random.normal() \n",
    "Wo1 = np.random.normal(size=3)\n",
    "Wo2 = np.random.normal(size=3)\n",
    "bo1 = np.random.normal() \n",
    "bo2 = np.random.normal()\n",
    "\n",
    "X = [1, 0, 1, 0]\n",
    "Y = G(X, Wh1, Wh2, Wh3, bh1, bh2, bh3, Wo1, Wo2, bo1, bo2)\n",
    "print(\"X=\" + str(X) + \"  ->  Y=\" + str(Y))\n",
    "X = [1, 1, 1, 1]\n",
    "Y = G(X, Wh1, Wh2, Wh3, bh1, bh2, bh3, Wo1, Wo2, bo1, bo2)\n",
    "print(\"X=\" + str(X) + \"  ->  Y=\" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, [0, 0, 1], [0, 0, 1], [0, 0]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(step(0.5))\n",
    "print(step([-1,0,1]))\n",
    "print(step(np.array([-1,0,1])))\n",
    "print(step((-1,0,1)))\n",
    "\n",
    "step([0.5, [-1,0,1], np.array([-1,0,1]), (-1,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, [0, 0, 1], [0, 0, 1], [0, 0]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step(z):\n",
    "    if isinstance(z, (list, np.ndarray, tuple)):\n",
    "        return [step(zi) for zi in z]\n",
    "    \n",
    "    if z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "print(step(0.5))\n",
    "print(step([-1,0,1]))\n",
    "print(step(np.array([-1,0,1])))\n",
    "print(step((-1,0,1)))\n",
    "\n",
    "step([0.5, [-1,0,1], np.array([-1,0,1]), (-1,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
