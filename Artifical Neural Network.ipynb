{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Neuron notebook we looked at the single neuron and how it could be simulated given input `X`, weights `W`, bias `b` and an activation function `f`. We saw how we could reduce the number of lines of code using vector operations and how by showing the weights and bias in a clever way could make the neuron calculate the function we wanted in a few cases. We also managed to connect multiple neurons and make the network calculate the `XOR` of 2 inputs, something not possible with a single neuron.\n",
    "\n",
    "It was a bit of a hassle and quite a few lines of code with the small network and to verify that it did what we wanted. The goal in this notebook is to be able to calculate the output of a Artificial Neural Network in a easier way than before, and also to be able to verify how correct the output is. We will still do the maths ourselves, using vector operations and the numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have an input layer with 4 inputs, a hidden layer with 3 neurons and an output layer with 2 neurons. We use `X` for the input values as before and `Y` for the output result. Ideally we would like to use `W` to represent the weights and `b` for the bias, but now we have two layers of neurons with multiple number of neurons in each layer, so we must make sure we don't confuse ourselves to much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation we could use the following variables:\n",
    "\n",
    "```python\n",
    "X = [x1, x2, x3, x4]  # the inputs\n",
    "Wh1 = [?, ?, ?, ?]    # the weights of the first neuron in the hidden layer connected to all 4 inputs\n",
    "Wh2 = [?, ?, ?, ?]    # the weights of the second neuron in the hidden layer connected to all 4 inputs\n",
    "Wh3 = [?, ?, ?, ?]    # the weights of the third neuron in the hidden layer connected to all 4 inputs\n",
    "Wo1 = [?, ?, ?]    # the weights of the first neuron in the output layer connected to all 3 hidden neurons\n",
    "Wo2 = [?, ?, ?]    # the weights of the second neuron in the output layer connected to all 3 hidden neurons\n",
    "bh1, bh2, bh3, bo1, bo2 = ? # the bias of our 5 neurons\n",
    "```\n",
    "\n",
    "To calculate the the output from the first neuron in the hidden layer we could do:\n",
    "\n",
    "```python\n",
    "zh1 = np.dot(X, W) + b\n",
    "h1 = step(zh1)\n",
    "```\n",
    "\n",
    "It would be a bit laboursome to type this code for all the neurons though, and it wouldn't scale well. So, how can we do it more convenient, but still very clear. We could do similar to in the Neuron notebook:\n",
    "\n",
    "```python\n",
    "def g(X, W):\n",
    "    z = np.dot(X, W)\n",
    "    return step(z)\n",
    "```\n",
    "\n",
    "and we could create a function that evaluate the full network like this:\n",
    "\n",
    "```python\n",
    "def G(X, W1, W2, W3):\n",
    "    h1 = g(X, W1)\n",
    "    h2 = g(X, W2)\n",
    "    y = g([h1, h2], W3)\n",
    "    return y\n",
    "```\n",
    "\n",
    "Note that we did not need the bias in that example and therefore it is skipped in the above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to create thos kind of functions for our current network. (By accident we forgot to consider the usage for this network, but we will ignore that for now and focuse on the structure and the code to simulate it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(z):\n",
    "    return 1 if z > 0 else 0\n",
    "\n",
    "def g(X, W, b):\n",
    "    z = np.dot(X, W) + b\n",
    "    return step(z)\n",
    "\n",
    "def G(X, Wh1, Wh2, Wh3, bh1, bh2, bh3, Wo1, Wo2, bo1, bo2):\n",
    "    h1 = g(X, Wh1, bh1)\n",
    "    h2 = g(X, Wh2, bh2)\n",
    "    h3 = g(X, Wh3, bh3)\n",
    "    h = [h1, h2, h3]\n",
    "    y1 = g(h, Wo1, bo1)\n",
    "    y2 = g(h, Wo2, bo2)\n",
    "    y = [y1, y2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully `G` now correctly calculates the output of our 3 layer network (an input layer with 4 inputs, a hidden layer with 3 neurons and an output layer with 2 neurons), but yikes are there many parameters. Lets try to call the `G` and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=[1, 0, 1, 0]  ->  Y=[0, 1]\n",
      "X=[1, 1, 1, 1]  ->  Y=[1, 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # to have deterministic random values :-)\n",
    "Wh1 = np.random.normal(size=4)\n",
    "Wh2 = np.random.normal(size=4)\n",
    "Wh3 = np.random.normal(size=4)\n",
    "bh1 = np.random.normal()\n",
    "bh2 = np.random.normal()\n",
    "bh3 = np.random.normal() \n",
    "Wo1 = np.random.normal(size=3)\n",
    "Wo2 = np.random.normal(size=3)\n",
    "bo1 = np.random.normal() \n",
    "bo2 = np.random.normal()\n",
    "\n",
    "X = [1, 0, 1, 0]\n",
    "Y = G(X, Wh1, Wh2, Wh3, bh1, bh2, bh3, Wo1, Wo2, bo1, bo2)\n",
    "print(\"X=\" + str(X) + \"  ->  Y=\" + str(Y))\n",
    "X = [1, 1, 1, 1]\n",
    "Y = G(X, Wh1, Wh2, Wh3, bh1, bh2, bh3, Wo1, Wo2, bo1, bo2)\n",
    "print(\"X=\" + str(X) + \"  ->  Y=\" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seemed to work, but all those parameters, and that handcrafted network function `G`, there must be a way to simplify that. Lets consider if we can group the neurons in the same layer together and calculate one layer at a time by using vectorisation and matrix multiplications. (We will skip the theoretical evidence part for now. :-)\n",
    "\n",
    "To simplify things lets for a moment consider a network with 2 layers, one input layer with 2 inputs and one output layer with 2 neurons. This means we should be able to calculate the outputs like this:\n",
    "\n",
    "```python\n",
    "y1 = step(np.dot(X, W1) + b1)\n",
    "y2 = step(np.dot(X, W2) + b2)\n",
    "```\n",
    "\n",
    "The question then becomes if we could combine the calculation of y1 and y2 in a way that scales in both readability and performance (there could be 100s of layers with 1000s of neurons or more in total).\n",
    "\n",
    "Lets say we want this small network to implement the following truth table, to make sure we can verify our magic simplification we are about to do:\n",
    "\n",
    " x1 | x2 | y1 | y2\n",
    "----|----|----|----\n",
    " 0  | 0  | 0  | 0\n",
    " 0  | 1  | 0  | 1\n",
    " 1  | 0  | 0  | 1\n",
    " 1  | 1  | 1  | 1\n",
    " \n",
    "This means that neuron 1 is supposed to be an `AND` operation and neuron 2 an `OR` operation.\n",
    "\n",
    "Lets first do it the old laboursome way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(X):\n",
    "    W1 = [1, 1]  # Lets hide the weights and bias inside our network function for this specifik network\n",
    "    b1 = -1\n",
    "    W2 = [1, 1]\n",
    "    b2 = 0\n",
    "\n",
    "    y1 = step(np.dot(X, W1) + b1)\n",
    "    y2 = step(np.dot(X, W2) + b2)\n",
    "\n",
    "    return [y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 1], [0, 1], [1, 1]]\n",
      "[True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    ([0,0], [0,0]),\n",
    "    ([0,1], [0,1]),\n",
    "    ([1,0], [0,1]),\n",
    "    ([1,1], [1,1])]\n",
    "\n",
    "print([G(X) for X,_ in examples])  # Prints the outputs for all our four examples\n",
    "\n",
    "print([G(X)==Y for X,Y in examples]) # Should output [True, True, True, True] if it matches our truth table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network function now does what it is supposed to, and we have some magic code to verify that it is correct. Now it is time for some speculative refactoring and trying to use matrix multiplications.\n",
    "\n",
    "```python\n",
    "#Current version\n",
    "def G(X):\n",
    "    W1 = [1, 1]  # Lets hide the weights and bias inside our network function for this specifik network\n",
    "    b1 = -1\n",
    "    W2 = [1, 1]\n",
    "    b2 = 0\n",
    "\n",
    "    y1 = step(np.dot(X, W1) + b1)\n",
    "    y2 = step(np.dot(X, W2) + b2)\n",
    "\n",
    "    return [y1, y2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 1.0]]\n",
      "[True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# Potentially optimised version\n",
    "def G(X):\n",
    "    W1 = [1, 1]  # Lets hide the weights and bias inside our network function for this specifik network    \n",
    "    W2 = [1, 1]\n",
    "    b1 = -1\n",
    "    b2 = 0\n",
    "    \n",
    "    W = np.array([W1,W2], ndmin=2).T\n",
    "    B = np.array([b1,b2], ndmin=2)\n",
    "    \n",
    "    Z = np.dot(X, W) + B\n",
    "\n",
    "    Y = [step(z) for z in Z]\n",
    "    \n",
    "    return Y[0] # to make sure the output is same type as it was before\n",
    "\n",
    "print([G(X) for X,_ in examples])  # Prints the outputs for all our four examples\n",
    "\n",
    "print([G(X)==Y for X,Y in examples]) # Should output [True, True, True, True] if it matches our truth table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests still holds, so it seems like the new version is actually implementing same functinality as the old version, but what is really happening? Lets take a look at the math here. \n",
    "\n",
    "Before we had one array with two weights per neuron, but know we combine them into a 2 x 2 matrix. Before we had:\n",
    "\n",
    "```python\n",
    "W1 = [w11, w12]  # The weights for neuron one\n",
    "W2 = [w21, w22]  # The weights for neuron two\n",
    "b1 = -1\n",
    "b2 = 0\n",
    "```\n",
    "\n",
    "But now we combine them into:\n",
    "\n",
    "```python\n",
    "W = [\n",
    "    [w11, w21],\n",
    "    [w12, w22]\n",
    "]\n",
    "B = [b1, b2]\n",
    "X = [x1, x2]\n",
    "Z = np.dot(X, W) + B # = [z1, z2]\n",
    "```\n",
    "\n",
    "If all of this seems a bit confusing, don't worry to much, the intention is to learn a few thing about python and how operations with matrices can be used to simulate artifical neural networks (ANNs). The end goal is to have a library of functions that can be used to create and use ANNs and to know how to use them and for that it might help to know something about how they could be implemented.\n",
    "\n",
    "So, lets go back and try to refactor that old network function for our initially planned network with 3 layers and 5 neurons. What we have since before is:\n",
    "\n",
    "```python\n",
    "def step(z):\n",
    "    return 1 if z > 0 else 0\n",
    "\n",
    "def g(X, W, b):\n",
    "    z = np.dot(X, W) + b\n",
    "    return step(z)\n",
    "\n",
    "def G(X, Wh1, Wh2, Wh3, bh1, bh2, bh3, Wo1, Wo2, bo1, bo2):\n",
    "    h1 = g(X, Wh1, bh1)\n",
    "    h2 = g(X, Wh2, bh2)\n",
    "    h3 = g(X, Wh3, bh3)\n",
    "    h = [h1, h2, h3]\n",
    "    y1 = g(h, Wo1, bo1)\n",
    "    y2 = g(h, Wo2, bo2)\n",
    "    y = [y1, y2]\n",
    "    return y\n",
    "```\n",
    "\n",
    "Lets now try to redo it using matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First a version of the step function that also can be applied to an array, matrix or whatever we might want to throw at it\n",
    "def step(z):\n",
    "    if isinstance(z, (list, np.ndarray, tuple)):\n",
    "        return [step(zi) for zi in z]\n",
    "    else:\n",
    "        return 1 if z > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try the network function G\n",
    "# X are the inputs as before\n",
    "# L1 are the weights and bias for layer 1 as a tuple\n",
    "# L2 are the weights and bias for layer 2 as a tuple\n",
    "def G(X, L1, L2):\n",
    "    W1, B1 = L1\n",
    "    W2, B2 = L2\n",
    "    \n",
    "    H = step(np.dot(X, W1) + B1)  # Output from the hidden layer\n",
    "    Y = step(np.dot(H, W2) + B2)  # Output from the network\n",
    "       \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above code actualluy works, it is a major cleanup of the previous version, but how should we test it? Maybe us it to implement a binary adder:\n",
    "\n",
    "x1 | x2 | y1 | y2\n",
    "---|----|----|---\n",
    " 0 |  0 |  0 |  0\n",
    " 0 |  1 |  0 |  1\n",
    " 1 |  0 |  0 |  1\n",
    " 1 |  1 |  1 |  0\n",
    " \n",
    "How should we break this down then so we can figure out the weights and bias? Well, looking at the truth table it seems like the following equations should work.\n",
    "\n",
    "```\n",
    "y1 = x1 and x2\n",
    "y2 = (x1 or x2) and not (x1 and x2)\n",
    "```\n",
    "\n",
    "Can we fit this into the above network function `G` expecting 3 layers? It seems we can split the equations into:\n",
    "\n",
    "```\n",
    "h1 = x1 and x2\n",
    "h2 = x1  or x2\n",
    "y1 = h1\n",
    "y2 = h2 and not h1\n",
    "```\n",
    "\n",
    "Feeling up to figuring out the needed weights and biases? Or do you long for some training algorithm to find them? ;-) For now we will handcode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 1], [0, 1], [1, 0]]\n",
      "[True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "L1 = ([[1,  1], \n",
    "       [1,  1]], \n",
    "      [-1,  0])\n",
    "L2 = ([[1, -1], \n",
    "       [0,  1]], \n",
    "      [ 0,  0])\n",
    "\n",
    "examples = [\n",
    "    ([0, 0], [0, 0]),\n",
    "    ([0, 1], [0, 1]),\n",
    "    ([1, 0], [0, 1]),\n",
    "    ([1, 1], [1, 0])\n",
    "]\n",
    "\n",
    "print([G(X, L1, L2) for X, _ in examples])\n",
    "print([G(X, L1, L2) == Y for X, Y in examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the output matches what we wanted. I don't know about you, but at least I need a break. If you actually understand why the selected weights and biases workes out in the code above, that is impressive, congratulate yourself. If you don't get it to add up, well, I had to fiddle abit to get it to work out, so don't worry to much about it. We will look into training a neetwork later. But first that break."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
