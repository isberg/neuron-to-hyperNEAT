{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neurons used in Artificial Neural Networks (ANNs) are a simplified simulation of how the neurons in our brains work. (Artificial) Neurons have inputs and an output as well as an activation function. Lets start with simulating a Neuron with two inputs and an output. The letter x is often used for inputs and y for outputs. You will also see that there are weights (w) and a bias (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume the inputs and parameters to our neuron is as follows:\n",
    "\n",
    "| x1 | x2 | w1 | w2 | b |\n",
    "|----|----|----|----|---|\n",
    "| 1  | 1  | 1  | 0.5| ? |\n",
    "\n",
    "We can describe that in Python like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 1.0\n",
    "x2 = 1.0\n",
    "w1 = 1.0\n",
    "w2 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can calculate the output as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x1 * w1 + x2 * w2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we change the input values then, lets say change x1 from 1 to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 0.0\n",
    "y = x1 * w1 + x2 * w2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is all very well to be able to do these above calculations, but it seems a bit laboursome to write this code over and over again, and what about the actiation function, and the offset b? Lets back up and introduce those concepts and define an upgraded formula for the neuron. Lets call the activation function f.\n",
    "\n",
    "```python\n",
    "z = x1 * w1 + x2 * w2 + b\n",
    "y = f(z)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "z = x1 * w1 + x2 * w2 + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we set b to 0 and calculated z, but what should f be then? Traditionally you want the neuron to fire (output something close to 1) if the sum of the inputs are high enough and not fire if the inputs are low (outputting something close to 0) and possibly outputting something between 0 and 1 if the inputs are somewhere in between. Traditionally a sigmoid function is used, but for simplicity, we will instead use a step function and define it as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put all this together in a neuron function and see if we can do something useful with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(x1, x2, w1, w2, b, f):\n",
    "    z = x1 * w1 + x2 * w2 + b\n",
    "    y = f(z)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to call it with a few different values and see what happens. Maybe use these values:\n",
    "\n",
    "| x1 | x2 | w1 | w2 | b | f |\n",
    "|----|----|----|----|---|---|\n",
    "| 1  | 1  | 1  | 0.5| 0 |step|\n",
    "| 0  | 1  | 1  | 0.5| 0 |step|\n",
    "| 0  | 0  | 1  | 0.5| 0 |step|\n",
    "| 0  | 0  | 1  | 0.5| 0.5 |step|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(neuron(1, 1, 1, 0.5, 0, step))\n",
    "print(neuron(0, 1, 1, 0.5, 0, step))\n",
    "print(neuron(0, 0, 1, 0.5, 0, step))\n",
    "print(neuron(0, 0, 1, 0.5, 0.5, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressed? What have we achieved? Or rather, what do we want to achieve? Perhaps we, for starters want to achieve something that given two inputs give the one output according to this:\n",
    "\n",
    "| x1 | x2 | y  |\n",
    "|----|----|----|\n",
    "| 0  | 0  | 0  |\n",
    "| 0  | 1  | 0  |\n",
    "| 1  | 0  | 0  | \n",
    "| 1  | 1  | 1  | \n",
    "\n",
    "When both inputs are 1 then we want the output to be one, otherwise 0. The question is then what we should use as weights, bias and activation function to match this table close enough. (Btw, the table is actually the truth table for logical AND.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.6\n",
    "w2 = 0.6\n",
    "b = -1\n",
    "print(neuron(0, 0, w1, w2, b, step))\n",
    "print(neuron(0, 1, w1, w2, b, step))\n",
    "print(neuron(1, 0, w1, w2, b, step))\n",
    "print(neuron(1, 1, w1, w2, b, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want our neuron friend to calculate the following table then:\n",
    "\n",
    "| x1 | x2 | y  |\n",
    "|----|----|----|\n",
    "| 0  | 0  | 0  |\n",
    "| 0  | 1  | 1  |\n",
    "| 1  | 0  | 1  | \n",
    "| 1  | 1  | 1  | \n",
    "\n",
    "What values should w1, w2 and b have then? (Assume that we still use out step function.)\n",
    "\n",
    "For convenience the definition of the functions for neutron and step is repeated here:\n",
    "\n",
    "```python\n",
    "def neuron(x1, x2, w1, w2, b, f):\n",
    "    z = x1 * w1 + x2 * w2 + b\n",
    "    y = f(z)\n",
    "    return y \n",
    "\n",
    "def step(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "```\n",
    "\n",
    "This means that a call like `neuron(0, 1, w1, w2, b, step)` will lead to `step(0 * w1 + 1 * w2 + b)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "w1 = 42\n",
    "w2 = 54\n",
    "b = 1337\n",
    "print(neuron(0, 0, w1, w2, b, step))  # = step(0 * w1 + 0 * w2 + b) = step(b)\n",
    "print(neuron(0, 1, w1, w2, b, step))  # = step(0 * w1 + 1 * w2 + b) = step(w2 + b)\n",
    "print(neuron(1, 0, w1, w2, b, step))  # = step(1 * w1 + 0 * w2 + b) = step(w1 + b)\n",
    "print(neuron(1, 1, w1, w2, b, step))  # = step(1 * w1 + 1 * w2 + b) = step(w1 + w2 + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output should be: \n",
    "\n",
    "0<br/>\n",
    "1<br/>\n",
    "1<br/>\n",
    "1<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all very well and hopefulle you have managed to tune the parameters (w1, w2, b) to make a new logical function (OR). It would be nice not having to manually tune these parameters though, but we will look into that a bit later. First we will see how hard it becomes if we want more inputs to our neuron and also look into a mathematical representation using arrays and matrices.\n",
    "\n",
    "If we have 3 inputs the equations will become:\n",
    "\n",
    "```python\n",
    "z = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "y = f(z)\n",
    "```\n",
    "\n",
    "As it happens we can actually represent this as vector operations instead, like:\n",
    "\n",
    "```\n",
    "z = X * W + b\n",
    "y = f(z)\n",
    "```\n",
    "\n",
    "X and W are vectors of decimal numbers, represented with capital letters, z, y and b are decimal numbers, represented with small letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1.0, 1.0]\n",
    "W = [0.6, 0.6]\n",
    "b = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out the vector multiplication in a simple way we import the `numpy` package and use the `dot` operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = np.dot(X, W) + b = 0.19999999999999996\n",
      "y = step(z) = 1\n"
     ]
    }
   ],
   "source": [
    "z = np.dot(X, W) + b\n",
    "print(\"z = np.dot(X, W) + b = \" + str(z))\n",
    "y = step(z)\n",
    "print(\"y = step(z) = \" + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What wast the point of all this you might wonder? Lets consider you have a neuron with 4 inputs, meaning 4 weights w1-w4 and a bias b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [0.11648627 0.03456871 0.14740154 0.4185378 ]\n",
      "b = 0.8293767926686777\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(4)\n",
    "b = np.random.rand()\n",
    "print(\"W = \" + str(W))\n",
    "print(\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the formula from above\n",
    "\n",
    "```\n",
    "z = X * W + b\n",
    "y = f(z)\n",
    "```\n",
    "\n",
    "but change it to python using the numpy library\n",
    "\n",
    "```python\n",
    "z = np.dot(X, W) + b\n",
    "y = step(z)\n",
    "```\n",
    "\n",
    "We also need to give some values to the 4 inputs represented by the vector X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [-1, 0, 1, 0.5]\n",
      "W = [0.11648627 0.03456871 0.14740154 0.4185378 ]\n",
      "z = np.dot(X, W) + b = 1.0695609715658734\n",
      "y = step(z) = step(1.0695609715658734) = 1\n"
     ]
    }
   ],
   "source": [
    "X = [-1, 0, 1, 0.5]\n",
    "z = np.dot(X, W) + b\n",
    "y = step(z)\n",
    "print(\"X = \" + str(X))\n",
    "print(\"W = \" + str(W))\n",
    "print(\"z = np.dot(X, W) + b = \" + str(z))\n",
    "print(\"y = step(z) = step(\" + str(z) + \") = \" + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of using vectors W and X instead of multiple variables x1-xn and w1-wn is to make the code more flexible and simplify things. It does however become a bit more abstract, but it is needed if we want a simple way to handly multiple neurons with a flexible number of inputs.\n",
    "\n",
    "Lets complicate things a bit and try to solve a problem that requires more than the single neuron. Well, you will really be handed the solution because the point right now is to show the mathematics and programming that makes it simple to handle multiple neurons instead of the one, at least if they are in a nice an structured way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the task to use neurons to implement the following function:\n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 0 |\n",
    "\n",
    "If one of the inputs are one, then the output should be one, otherwise the output should be zero. Also known as Exclusive OR, or XOR. This cannot be solved by a single normal neuron and are useful to force a multi-neuron solution.\n",
    "\n",
    "How do we start then? You have previously used a single neuron to implent first AND, then OR, so if we can put those together somehow and make XOR, then we should be able to make it. \n",
    "\n",
    "Luckily, XOR could be expressed as:\n",
    "\n",
    "```\n",
    "A xor B = ((not A) and B) or (A and (not B)) \n",
    "```\n",
    "\n",
    "If we could find a way to make a neuron calculate `(not A) and B`, then we can also calculate `A and (not B)` in a similar way and just feed those outputs into a neuron calculating `or`, as we have done before. It might not be the minimal solution, but there are some structure to it, and we will use that structure togehter with multi-dimensional arrays to simulate a neural network with 2 inputs, 3 neurons and 1 input in an amazingly few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with `(not A) and B` but instead of A and B we use x1 and x2 as before and look at the truth table we want our first neuron to implement. Lets also name the output of this first (hidden) neuron h1. We will use y for the final output of the full three neuron network.\n",
    "\n",
    "| x1 | x2 | h1 |\n",
    "|----|----|----|\n",
    "| 0  | 0  | 0  |\n",
    "| 0  | 1  | 1  |\n",
    "| 1  | 0  | 0  |\n",
    "| 1  | 1  | 0  |\n",
    "\n",
    "What should our weight and bias be then? Lets name them W1 and b1. There are multiple algorithms to find these parameters, all of which are out of scope right now, but at least some of them is some sort of try, see how far off, and try to adjust in some more or less magic way. Lets start with grabbing random parameters and see how far off we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [0.57164302 0.08728161]\n",
      "b1 = 0.768254510315076\n",
      "[0, 0] -> 1, 0 expected\n",
      "[0, 1] -> 1, 1 expected\n",
      "[1, 0] -> 1, 0 expected\n",
      "[1, 1] -> 1, 0 expected\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.rand(2)\n",
    "b1 = np.random.rand()\n",
    "print(\"W1 = \" + str(W1))\n",
    "print(\"b1 = \" + str(b1))\n",
    "X = [0, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 0 expected\")\n",
    "X = [0, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 1 expected\")\n",
    "X = [1, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 0 expected\")\n",
    "X = [1, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 0 expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the following output, which does not match our goal. Your result might differ due to randomizing the weights and bias.\n",
    "\n",
    "```\n",
    "W1 = [0.57164302 0.08728161]\n",
    "b1 = 0.768254510315076\n",
    "[0, 0] -> 1, 0 expected\n",
    "[0, 1] -> 1, 1 expected\n",
    "[1, 0] -> 1, 0 expected\n",
    "[1, 1] -> 1, 0 expected\n",
    "```\n",
    "\n",
    "It was not really expected that it should be correct either, so lets use some magic inspiration to try to tweak the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [-1, 1]\n",
      "b1 = 0\n",
      "[0, 0] -> 0, 0 expected\n",
      "[0, 1] -> 1, 1 expected\n",
      "[1, 0] -> 0, 0 expected\n",
      "[1, 1] -> 0, 0 expected\n"
     ]
    }
   ],
   "source": [
    "W1 = [-1, 1]\n",
    "b1 = 0\n",
    "print(\"W1 = \" + str(W1))\n",
    "print(\"b1 = \" + str(b1))\n",
    "X = [0, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 0 expected\")\n",
    "X = [0, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 1 expected\")\n",
    "X = [1, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 0 expected\")\n",
    "X = [1, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W1) + b1)) + \", 0 expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output is:\n",
    "\n",
    "```\n",
    "W1 = [-1, 1]\n",
    "b1 = 0\n",
    "[0, 0] -> 0, 0 expected\n",
    "[0, 1] -> 1, 1 expected\n",
    "[1, 0] -> 0, 0 expected\n",
    "[1, 1] -> 0, 0 expected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets handle the next part of our equation then, the `A and (not B)` part, but again swapping to x1 and y1 and name the output h2 and the weights and bias W2 and b2 this time.\n",
    "\n",
    "| x1 | x2 | h1 | h2 |\n",
    "|----|----|----|----|\n",
    "| 0  | 0  | 0  | 0  |\n",
    "| 0  | 1  | 1  | 0  |\n",
    "| 1  | 0  | 0  | 1  |\n",
    "| 1  | 1  | 0  | 0  |\n",
    "\n",
    "Can you figure this out yourself, based on that is pretty much similar to the last one, except that the `not` part moved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 = [0.75550366 0.20327929] # random values here wont cut it. ;-)\n",
      "b2 = 0.500674277379526      # random values here wont cut it. ;-)\n",
      "[0, 0] -> 1, 0 expected\n",
      "[0, 1] -> 1, 0 expected\n",
      "[1, 0] -> 1, 1 expected\n",
      "[1, 1] -> 1, 0 expected\n"
     ]
    }
   ],
   "source": [
    "W2 = np.random.rand(2)\n",
    "b2 = np.random.rand()\n",
    "print(\"W2 = \" + str(W2) + \" # random values here wont cut it. ;-)\")\n",
    "print(\"b2 = \" + str(b2) + \"      # random values here wont cut it. ;-)\")\n",
    "X = [0, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W2) + b2)) + \", 0 expected\")\n",
    "X = [0, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W2) + b2)) + \", 0 expected\")\n",
    "X = [1, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W2) + b2)) + \", 1 expected\")\n",
    "X = [1, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W2) + b2)) + \", 0 expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you figured out the weights and bias for the second neuron now we just have the final one, that will combine the outputs from neuron 1 and 2 using OR, which we previously have implemented far above. The truth table for OR looks like this:\n",
    "\n",
    "| h1 | h2 | y  |\n",
    "|----|----|----|\n",
    "| 0  | 0  | 0  |\n",
    "| 0  | 1  | 1  |\n",
    "| 1  | 0  | 1  | \n",
    "| 1  | 1  | 1  | \n",
    "\n",
    "Given the outputs from the 2 first (hidden) neurons, the last neuron will calculation the output from the artificial neural network (ANN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W3 = [0.77273072 0.47884273] # random values here wont cut it. ;-)\n",
      "b3 = 0.11031243938640378      # random values here wont cut it. ;-)\n",
      "[0, 0] -> 1, 0 expected\n",
      "[0, 1] -> 1, 1 expected\n",
      "[1, 0] -> 1, 1 expected\n",
      "[1, 1] -> 1, 1 expected\n"
     ]
    }
   ],
   "source": [
    "W3 = np.random.rand(2)\n",
    "b3 = np.random.rand()\n",
    "print(\"W3 = \" + str(W3) + \" # random values here wont cut it. ;-)\")\n",
    "print(\"b3 = \" + str(b3) + \"      # random values here wont cut it. ;-)\")\n",
    "X = [0, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W3) + b3)) + \", 0 expected\")\n",
    "X = [0, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W3) + b3)) + \", 1 expected\")\n",
    "X = [1, 0]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W3) + b3)) + \", 1 expected\")\n",
    "X = [1, 1]\n",
    "print(str(X) + \" -> \" + str(step(np.dot(X, W3) + b3)) + \", 1 expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have figured out the parameters for the output neuron, we have the following situation.\n",
    "\n",
    "Neuron # | Type       |Input from | Output to | Weights | bias\n",
    "---------|------------|-----------|-----------|---------|-----\n",
    "x1       |  Input     |           |  1 and 2  |         | \n",
    "x2       |  Input     |           |  1 and 2  |         |  \n",
    "1        |  Hidden    |  x1 and x2|  3        | -1,  1  | 0\n",
    "2        |  Hidden    |  x1 and x2|  3        |  1, -1  | 0\n",
    "3        |  Output    |  1 and 2  |  (y)      |  1,  1  | 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = [-1,  1]\n",
    "W2 = [ 1, -1]\n",
    "W3 = [ 1,  1]\n",
    "b  = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have used the step function as activation function and that the bias actually could be set to 0 for all neurons in our xor case, we can define a function that calculation the output from a neuron given the inputs and the weights in this way:\n",
    "\n",
    "```python\n",
    "def g(X, W):\n",
    "    z = np.dot(X, W)\n",
    "    return step(z)\n",
    "```\n",
    "\n",
    "and we could create a function that evaluate the full network like this:\n",
    "\n",
    "```python\n",
    "def G(X, W1, W2, W3):\n",
    "    h1 = g(X, W1)\n",
    "    h2 = g(X, W2)\n",
    "    y = g([h1, h2], W3)\n",
    "    return y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(X, W):\n",
    "    z = np.dot(X, W)\n",
    "    return step(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(X, W1, W2, W3):\n",
    "    h1 = g(X, W1)\n",
    "    h2 = g(X, W2)\n",
    "    y = g([h1, h2], W3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X = [0, 0]\n",
    "print(G(X, W1, W2, W3))\n",
    "X = [0, 1]\n",
    "print(G(X, W1, W2, W3))\n",
    "X = [1, 0]\n",
    "print(G(X, W1, W2, W3))\n",
    "X = [1, 1]\n",
    "print(G(X, W1, W2, W3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we make it? Lets compare with the truth table we wanted to implement.\n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrats on reaching the end. I hope you learned something about artifical neurons and how the can be composed into a network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
