{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Artificial Neural Network notebook we saw how we could use operations on matrices to to handle a layer of neurons at a time. We also used some snippet of code to verify if the network actually did what we expected it to.\n",
    "\n",
    "In this notebook we will start with looking at the matrix operations needed to evaluate a multi-layer artificial neural network (ANN). Then we will consider how we could calculate how close the ANN is to what we want it to be and finally we will look into how we could make it learn and reduce the error of its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to import and use the numpy library for vector/matrix/array operations, so we better import it\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets imagine we have a multi-layer ANN where the input come from the left, from the input layer named `X`, then we have a series of layers of neurons named `L1`-`Ln`, where all except the last layer `Ln` are hidden and we cannot observe the output. We call the expected output from layer n (`Ln`) for `Y`, and the actual output for `Yhat`. We use both `Y` and `Yhat` so we can calculate the error of the network when we try to improve it by updating the parameters (weights and biases).\n",
    "\n",
    "```\n",
    "X -> L1 -> L2 -> ... -> Li -> ... -> Ln-1 -> Ln -> Yhat\n",
    "```\n",
    "\n",
    "If we need to refer to the output for any hidden layer we use `H1` for the output of layer `L1` and so on. All of the layers can contain mutiple nodes (inputs or neurons). As we did in the last notebook, we want it to be easy to evaluate a full network layer at a time, but we would also like to evaluate mutiple example inputs at a time without needing to loop. Let see what we can come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
      "W = [[1, 1], [1, 1]]\n",
      "B = [0, -1]\n",
      "Z = \n",
      "[[ 0 -1]\n",
      " [ 1  0]\n",
      " [ 1  0]\n",
      " [ 2  1]]\n",
      "Yhat = [[0, 0], [1, 0], [1, 0], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "print(\"X = \" + str(X))\n",
    "W, B = L = ([[1, 1], [1, 1]], [0, -1])  # These parameters are meant to output OR from neuron 1 and AND from neuron 2\n",
    "print(\"W = \" + str(W))\n",
    "print(\"B = \" + str(B))\n",
    "# Would't it be dreamy if we could just calculate Z = X * W + B for all the example inputs in one go?\n",
    "# And then just run it all in ine go through an activation function f = step?\n",
    "Z = np.dot(X, W) + B\n",
    "print(\"Z = \\n\" + str(Z))\n",
    "\n",
    "def step(z):\n",
    "    if isinstance(z, (list, np.ndarray, tuple)):\n",
    "        return [step(zi) for zi in z]\n",
    "    else:\n",
    "        return 1 if z > 0 else 0\n",
    "    \n",
    "Yhat = step(Z)\n",
    "print(\"Yhat = \" + str(Yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look real close at the output (and think really hard) you might see that it actually matches a 2 layer network with 2 nodes in each layer that outputs `OR` from the first neuron and `AND` from the second neuron. It is kind of har to see it though. We did however manage to calculate the outputs from one layer for all our example inputs in one go though, so that is great. Lets look into the parameters in the form of weights and biases and see if it would be useful to rearrange them and possibly the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "W:\n",
      "[[10 10]\n",
      " [10 10]]\n",
      "B:\n",
      "[[ -5]\n",
      " [-15]]\n",
      "Z:\n",
      "[[ -5   5   5  15]\n",
      " [-15  -5  -5   5]]\n",
      "Yhat:\n",
      "[[0.01 0.99 0.99 1.  ]\n",
      " [0.   0.01 0.01 0.99]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # .T is numpys transpose operation, swapping rows with columns in the matrix\n",
    "print(\"X:\\n\" + str(X))\n",
    "\n",
    "W, B = L = (np.array([[10, 10], [10, 10]]),\n",
    "            np.array([[-5, -15]]).T)  # These parameters are meant to output OR from neuron 1 and AND from neuron 2\n",
    "\n",
    "print(\"W:\\n\" + str(W))\n",
    "print(\"B:\\n\" + str(B))\n",
    "\n",
    "Z = W.dot(X) + B\n",
    "print(\"Z:\\n\" + str(Z))\n",
    "\n",
    "sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
    "f = sigmoid\n",
    "\n",
    "Yhat = f(Z)\n",
    "print(\"Yhat:\\n\" + str(Yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we know have the input examples arranged with one example per column. We have the weights with one row per neuron. We have the bias with one neuron bias per row, mathching up with the row of weights. And finally the output matches the input with one column per example in the output. It is also formatted in a nicer way with long rows instead of columns and using numpy arrays everywhere. We also change secretely to use the sigmoid function, partly because we can apply that one to all elements of a matrix, which was not as handy with the step function we used before. Also the sigmoid function is more often used for neurons than the step function.\n",
    "\n",
    "Lets try to formalise these calculations into a network functions, calculating the outputs from a multi-layer ANN, taking the layers as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(inputs, layers, activationFunction):\n",
    "    activations = inputs\n",
    "    for layer in layers:\n",
    "        weights, bias = layer\n",
    "        Z = weights.dot(activations) + bias\n",
    "        activations = activationFunction(Z)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above network function uses verbose descriptive names rather than mathematical, you could also have it like below, depending on your preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(X, L, f):\n",
    "    A = X\n",
    "    for li in L:\n",
    "        W, B = li\n",
    "        Z = W.dot(A) + B\n",
    "        A = f(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are inerested in evaluating the performance of an ANN and improve the performance, it would be useful to calculate the error or loss of the network, as well as potentially determining the prediction rate if we use the network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Y W B:\n",
      " [[0 0 1 1]\n",
      " [0 1 0 1]] [[0 0 0 1]] [[-0.25 -1.62]] [[1.54]]\n",
      "Yhat: [[0.82 0.48 0.78 0.42]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # All possible binary inputs combinations using an input layer with 2 inputs\n",
    "Y = np.array([[0, 0, 0, 1]])  # Expected output, we want AND beaviour :-)\n",
    "\n",
    "W = np.random.randn(1,2)\n",
    "B = np.random.randn(1,1)\n",
    "print(\"X Y W B:\\n\", X, Y, W, B)\n",
    "\n",
    "Yhat = G(X, L = [(W,B)], f = sigmoid)  # Actual output\n",
    "print(\"Yhat:\", Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have both our expected output and our actual output we could calculate the difference in some clever way to see how well the network match what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta = Y - Yhat =  [[-0.82 -0.48 -0.78  0.58]]\n",
      "loss =  1.860642742045609\n"
     ]
    }
   ],
   "source": [
    "delta = Y - Yhat   # THe element-wise difference\n",
    "print(\"delta = Y - Yhat = \", delta)\n",
    "loss = np.sum(delta ** 2)    # square it (to remove the sign etc) and sum\n",
    "print(\"loss = \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compare different weights and bias it will be much easier to handle a single decimal number than an array of numbers do decide what is better. Let try to generate another set of parameters (W and B) and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Y W B:\n",
      " [[0 0 1 1]\n",
      " [0 1 0 1]] [[0 0 0 1]] [[-0.64 -0.31]] [[-0.56]]\n",
      "Yhat: [[0.36 0.3  0.23 0.18]]\n",
      "delta = Y - Yhat =  [[-0.36 -0.3  -0.23  0.82]]\n",
      "loss =  0.9430779061131485\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(1,2)\n",
    "B = np.random.randn(1,1)\n",
    "print(\"X Y W B:\\n\", X, Y, W, B)\n",
    "\n",
    "Yhat = G(X, L = [(W,B)], f = sigmoid)  # Actual output\n",
    "print(\"Yhat:\", Yhat)\n",
    "\n",
    "delta = Y - Yhat   # THe element-wise difference\n",
    "print(\"delta = Y - Yhat = \", delta)\n",
    "loss = np.sum(delta ** 2)    # square it (to remove the sign etc) and sum\n",
    "print(\"loss = \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in theory a smaller loss would mean the network is closer to implementing what we want, but it would also be nice to see how many of the examples it actually calculates the correct output for. To see how closely our network inplements an OR, we will count values above 0.5 as 1 and below as 0. We will then count the percentage of the examples it get right.\n",
    "\n",
    "To simplify life, we will now create helper functions to generate networks, evaluate network, calculate the loss and the correctness percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers L = (W , B) =  [(array([[ 0.29, -0.17]]), array([[-0.77]]))]\n"
     ]
    }
   ],
   "source": [
    "def generate(nodes):\n",
    "    layers = []\n",
    "    for i in range(len(nodes) - 1):\n",
    "        W = np.random.randn(nodes[i + 1], nodes[i])\n",
    "        B = np.random.randn(1,1)\n",
    "        layers.append((W,B))\n",
    "    return layers\n",
    "        \n",
    "L = generate([2, 1])\n",
    "print(\"Layers L = (W , B) = \", L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yhat =  [[0.63 0.77 0.77 0.87]]\n"
     ]
    }
   ],
   "source": [
    "def calculate(X, L, f):\n",
    "    A = X\n",
    "    for li in L:\n",
    "        W, B = li\n",
    "        Z = W.dot(A) + B\n",
    "        A = f(Z)\n",
    "    return A\n",
    "\n",
    "Yhat = calculate(X, L, sigmoid)\n",
    "print(\"Yhat = \", Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 % correct\n"
     ]
    }
   ],
   "source": [
    "def correctness(Yhat, Y):\n",
    "    interpret = (Yhat > 0.5) * 1\n",
    "    #print(interpret)\n",
    "    correct = interpret == Y\n",
    "    #print(correct)\n",
    "    correct_count = sum(correct.flatten())  # Yes, flatten is a bit fishy, it turns a matrix into a vector though, removing one dimension\n",
    "    return correct_count / len(Y.flatten())\n",
    "    \n",
    "result = correctness(Yhat, Y)\n",
    "print(result, \"% correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  0.5197701759851555\n"
     ]
    }
   ],
   "source": [
    "def loss(Yhat, Y):\n",
    "    return np.sum((Y - Yhat) ** 2)\n",
    "\n",
    "print(\"loss = \", loss(Yhat, Y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the possibility to generate, calculate the output from and calculate the loss and correctness of the output, lets see if we can manage to generate a network that implements a simple `OR` between 2 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network [(array([[ 1.44, -0.43]]), array([[-0.44]]))] gets 0.75 % correct.\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[0, 1, 1, 1]])\n",
    "\n",
    "L = generate([2, 1])\n",
    "Yhat = calculate(X, L, sigmoid)\n",
    "l = loss(Yhat, Y)\n",
    "score = correctness(Yhat, Y)\n",
    "print(\"Network\", L, \"gets\", score, \"% correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we use these functions to find a network that actually implements `OR`, lets see if we could randomly manage to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.75\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.25\n",
      "score: 0.75\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.5\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.5\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.75\n",
      "score: 0.5\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.25\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.75\n",
      "score: 0.75\n",
      "score: 0.5\n",
      "score: 0.5\n",
      "score: 0.75\n",
      "score: 0.75\n",
      "score: 0.75\n",
      "score: 0.75\n",
      "score: 0.75\n",
      "score: 0.5\n",
      "score: 0.0\n",
      "score: 0.0\n",
      "score: 0.75\n",
      "score: 0.25\n",
      "score: 0.25\n",
      "score: 0.0\n",
      "score: 0.5\n",
      "score: 1.0\n",
      "winner after 39 tries:  [(array([[0.82, 0.8 ]]), array([[-0.02]]))]\n",
      "[[0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    L = generate([2, 1])\n",
    "    Yhat = calculate(X, L, sigmoid)\n",
    "    score = correctness(Yhat, Y)\n",
    "    print(\"score:\", score)\n",
    "    if score == 1:\n",
    "        break\n",
    "        \n",
    "if score == 1:\n",
    "    print(\"winner after\", i, \"tries: \", L)\n",
    "    print((evaluate(X, L, sigmoid) > 0.5) * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work, however it is a bit random, and we probably should have a helper function to parse to output of the network to know if it closer to 0 or 1 (if those are the expected outputs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
