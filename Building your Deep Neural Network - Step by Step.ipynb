{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims, seed=None, randomize_bias = False, scaling = 0.01):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if (type(seed) is int ):\n",
    "        np.random.seed(seed)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * scaling\n",
    "        if randomize_bias:\n",
    "            parameters['b' + str(l)] = np.random.randn(layer_dims[l], 1) * scaling\n",
    "        else:\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1])) \n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.018]]),\n",
       " 'W2': array([[0.004]]),\n",
       " 'W3': array([[0.001]]),\n",
       " 'b1': array([[0.]]),\n",
       " 'b2': array([[0.]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters_deep(layer_dims=[1,1,1,1], seed = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z =  W.dot(A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_forward(A, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, hidden_activation='relu', output_activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, hidden_activation)\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W = parameters[\"W\" + str(L)]\n",
    "    b = parameters[\"b\" + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, output_activation)\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -1 / m * np.sum(Y * np.log(AL) + (1 - Y)*np.log(1-AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2628643221541276"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(np.array([[0.1,0.8]]), np.array([[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = 1 / m * dZ.dot(A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = W.T.dot(dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, hidden_activation='relu', output_activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, output_activation)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, hidden_activation)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the problem to solve\n",
    "def problem3xor():\n",
    "    X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]).T\n",
    "    Y = np.array([[   0,         1,         1,         0,         1,         0,         0,         1    ]])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the problem to solve\n",
    "def problem2xor():\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "    Y = np.array([[ 0,      1,      1,      0]])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the problem to solve\n",
    "def problem2bitand():\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "    Y = np.array([[0, 0], [0, 1], [0, 1], [1, 0]]).T\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0, cost = 0.6933896832030466, AL = [[0.488 0.502 0.497 0.513 0.491 0.507 0.501 0.519]]\n",
      "epoch # 1604, cost = 0.09984271774546626, \n",
      "AL = (array([[0.061, 0.973, 0.936, 0.06 , 0.838, 0.12 , 0.14 , 0.884]]), array([[0, 1, 1, 0, 1, 0, 0, 1]]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe0b7650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = problem3xor()\n",
    "\n",
    "layer_dims = [X.shape[0], 24, Y.shape[0]]\n",
    "randomize_bias = True\n",
    "scaling = 0.1\n",
    "learning_rate=0.1\n",
    "debug = True\n",
    "hidden_activation, output_activation = 'relu', 'sigmoid'\n",
    "seed=None#3 #108+4\n",
    "max_epochs = 500e3\n",
    "sigma = 0.1\n",
    "\n",
    "dot = ['r', 'g.', 'b.', 'y.', 'ro', 'go', 'bo', 'yo']\n",
    "\n",
    "parameters = initialize_parameters_deep(layer_dims, seed, randomize_bias, scaling)\n",
    "\n",
    "for epoch in range(int(max_epochs)):\n",
    "    AL, caches = L_model_forward(X, parameters, hidden_activation, output_activation)\n",
    "    cost = compute_cost(AL, Y)\n",
    "    if (epoch % 1000 == 0):\n",
    "        if debug and epoch % 10000 == 0: \n",
    "            print(\"epoch # {}, cost = {}, AL = {}\".format(epoch, cost, AL))\n",
    "        #plt.plot(epoch, cost, 'b.')\n",
    "        for i in range(AL.shape[1]):\n",
    "            plt.plot(epoch, AL[0][i], dot[i])\n",
    "    if (cost < sigma): \n",
    "        break\n",
    "    grads = L_model_backward(AL, Y, caches, hidden_activation, output_activation)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    \n",
    "#plt.plot(epoch, cost, 'g.')\n",
    "print(\"epoch # {}, cost = {}, \\nAL = {}\".format(epoch, cost, (AL, (AL>0.5)*1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
